{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef6f2b9-b475-47af-ac76-4bb1d4450d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b5ae72-5839-4bdf-98c0-5ec3110218a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amar from Punjab, Pakistan\n",
      "Can i scrap data of those website which are not created by Html\n",
      "Kon kon 2024 me dekh raha hai\n",
      "My name is Rain and I am from Bangladesh.\n",
      "Suleman Muhammad Khan\n",
      "Pakistan\n",
      "Sir ek machine learning ki full playlist provide kr dijiye apne &quot;‡§Ö‡§Ç‡§¶‡§æ‡§ú‡§º&quot; me\n",
      "Thanks CodeWithHarry. Bro, tum samaaj sewi ho!!! Keep it up!\n",
      "Hi Harry,<br>Your video is really impressive and it motivated me to understand the topic much deeper.<br>I have a request,Can i use your url for making my project on Web scraping?\n",
      "Hi bro its such a beautiful vedio...and you are doing a wonderful job..it helped me a lot to get my job...\n",
      "I need your help Harry\n",
      "I want to learn scrapping with beautiful soap Selenium and scrapy do you have any course available on your YouTube channel or you like to recommend me other way I have tried lots of ways but I am staking step by step in scrapping of websites what I am before at the same time and give response me that non sometime in Selenium when I run the program don&#39;t open website what I am do for it I don&#39;t improve so I need your help what you like to hear recommend me\n",
      "i am israr, from pakistan\n",
      "Muhammad Ajaib Hashmi Azad Kashmir\n",
      "Pakistan lahore Excellent video\n",
      "humayun from pakistan , islamabad\n",
      "I am Savnesh Daksh, a dedicated Data Scientist hailing from the vibrant city of Moradabad in Uttar Pradesh. Very help full vdo\n",
      "how many knows that The Real name of Harry Is HarisüòÇüòÇ\n",
      "Hi <br>I need help with my project web scraping .<br>Kindly reply.\n",
      "edu<br>yentha.pottu<br>boly\n",
      "nooooooooooooo id use agiain\n",
      "I am Hussain From Pakistan. Dear Harry You are absolutely a very intelligent and student friendly teacher, Your teaching style enable a raw handed person  to achieve the whole course of any language with keen interest.\n",
      "thanks a lot for this video! <br>from Bangalore\n",
      "india\n",
      "ya video bohat bohat helpful hai\n",
      "ya video bohat bohat helpful hai\n",
      "ya video bohat bohat helpful hai\n",
      "ya video bohat bohat helpful hai\n",
      "ya video bohat bohat helpful hai\n",
      "I am learning through this video and if i request again again to your code with harry website don&#39;t block my  IP address.....\n",
      "hey harry ..its mayur from chhatrapati sambhajinagar ,Maharashtra\n",
      "Rasmi from odisha\n",
      "From pakistan huzaifa.\n",
      "Gajab kya baat hai harry , aap to cha gaye . <b>Thoda sa prakash daalna chahunga</b>  üëå\n",
      "Hello Brother.<br>Anshul From Gurgoan.\n",
      "Dont waste time on this video.\n",
      "Shahid Pervaiz from Pakistan\n",
      "please make a new playlist for dsa in python\n",
      "‚ù§\n",
      "Jai Maharashtra bhai ‚ù§, Mumbai\n",
      "Im Jay. From Kharagpur\n",
      "Why someone would let their site scrape? It is like stealing their source code.\n",
      "from nepal and i am trying to book poland vfs slots now poland slot are close i wnt to book slot by using programming help\n",
      "How do u get the output in such a clean manner i tried the same code but my output was really cluttered\n",
      "Laila from Pakistan\n",
      "hello sir, my name is Ishaan I&#39;m from Jaipur,  Rajasthan,  India\n",
      "<a href=\"https://www.youtube.com/watch?v=uufDGjTuq34&amp;t=960\">16:00</a> print(soup.prettify())\n",
      "Awesome ‚ù§\n",
      "I am saddam from Pakistan ‚ù§\n",
      "THANK YOU BHAI !!!!!!!\n",
      "can we do it without librairy\n",
      "Majid From Pakistan\n",
      "good work\n",
      "Bhai is mae scraping kaha thi tags or class k naam utha k client ko den gay k ye andr tha?\n",
      "sir how to fetch or extract the sub webpage link of any website  which is which is having missing image means the product description is given but its not have image\n",
      "Mera Naam ‡§∂‡§ø‡§µ b\n",
      "brother how to get an element which is inside <a href=\"http://www.youtube.com/results?search_query=%23shadow\">#shadow</a>-root (open)\n",
      "use screen crab\n",
      "great video\n",
      "I am getting the output but it in horizontal form i want it in verticle so what should I do\n",
      "pakistan gujranwala\n",
      "from pakistan city lahore Kindly make one video related to chatbot\n",
      "Nice class\n",
      "is it possible to replace text only of dom elements without changing css attributes ?\n",
      "soup is giving the same result as htmlcontent, please help\n",
      "Som kumar from  Panta ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§\n",
      "Bro contact me mere paas automation ka project hai\n",
      "Udemy High Paid Web development course is Free on YouTube(Angela Yu)<br><a href=\"https://www.youtube.com/watch?v=TI4b0dwYFdg\">https://youtu.be/TI4b0dwYFdg</a>\n",
      "daksh from maharastra\n",
      "Very good video. Clear.\n",
      "bhai aapki aawaz farhan akhtar jaise hai üòÇ\n",
      "I am from new delhi\n",
      "I am using jupyter notebook to run this code, but at after one point my kernel keep going dead, showing error as &quot;&quot;the kernal appears to have died. It will restart automatically&quot;.<br><br>Please help.\n",
      "If I have to scrape data from amazon link, what should I use :<br> 1. API or <br>2. Web Scraping ?<br>Is the amazon provides API&#39;s ?\n",
      "I want to learn how to create an automated job website. Can you make a video of the website working like this via the API?\n",
      "Sir im from Pakistan Lahore<br>Sir me bhot time sw video dkh ra hu apki love u sir <br>Sir ik video api scaping py BNA doüòä\n",
      "Gourab Roy ,Ranchi\n",
      "from jaiur rajasthan\n",
      "from bs4 import BeautifulSoup giving me error @codeWithHarry\n",
      "bs4 is like vs code üòÉ\n",
      "noida\n",
      "Awesome‚ù§\n",
      "thank you\n",
      "lots of love from nepal . thank you sir\n",
      "Pawan Kumar from Kolkata .\n",
      "please make video on R language\n",
      "DELHI\n",
      "Utkarsh and I am from Vadodara\n",
      "I want to make a website &quot;It has one administrator login page and other is client logi page and the administrator update the website,upload the content on site and the clients can see the content and react,comment,give feedback  on the content &quot; this is what i want yo do ,now i don&#39;t know how to do all these things ,which language to use ,&quot; so if there is anyone who help me ,plz plz reply üëçüëçüëçüëç\n",
      "Anchor text at 24\n",
      "My name is Jitendra Parihar,<br>And I am from Rajasthan.\n",
      "Why my code is not running .<br><br>def add_no(a,b): <br>print(a+b) <br>return add_no <br><br>def mult_no(a,b)<br> print (a*b) <br>return mult_no <br><br>sum1 = add_no(a,b) <br>mul = mult_no(a,b) <br>sum1 (20,30)\n",
      "Sir is this(web scraping) illegal???\n",
      "Rajanish, Banglore\n",
      "Thank you! It was a great starting point to learn about web scrapping\n",
      "great job sir\n",
      "please,please,please,please,please,please,please,please,(Please make a video and explain about yourself specially qualification?üôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôèüôè‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§\n",
      "I am rahul raja forom  begusarai ,bihar.\n",
      "Lalit from Pune. loved the video.. so-so informative\n",
      "I am from Pakistan <br>I am a full-stack developer\n"
     ]
    }
   ],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyC_t1W-rv-JHoyIcPRypT6Asw0vTT5rB98\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=\"uufDGjTuq34\", # id of the video\n",
    "    maxResults=100\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "for item in response['items']:\n",
    "    print(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5528e-5dc6-4483-81ed-9add7b59575c",
   "metadata": {},
   "source": [
    "# output as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb7ec09-8b89-4a46-9b27-add0c06650fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@amariqbal</td>\n",
       "      <td>2024-05-05T11:43:18Z</td>\n",
       "      <td>2024-05-05T11:43:18Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Amar from Punjab, Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jishankhan6686</td>\n",
       "      <td>2024-04-03T09:33:44Z</td>\n",
       "      <td>2024-04-03T09:33:44Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Can i scrap data of those website which are no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Digitcoin150</td>\n",
       "      <td>2024-04-03T09:07:28Z</td>\n",
       "      <td>2024-04-03T09:07:28Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Kon kon 2024 me dekh raha hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@raingalaxy6594</td>\n",
       "      <td>2024-03-22T17:09:35Z</td>\n",
       "      <td>2024-03-22T17:09:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>My name is Rain and I am from Bangladesh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sulemanrana1480</td>\n",
       "      <td>2024-03-12T08:56:28Z</td>\n",
       "      <td>2024-03-12T08:56:28Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Suleman Muhammad Khan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@sulemanrana1480</td>\n",
       "      <td>2024-03-12T08:56:17Z</td>\n",
       "      <td>2024-03-12T08:56:17Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@rahulpathak8415</td>\n",
       "      <td>2024-03-08T18:15:53Z</td>\n",
       "      <td>2024-03-08T18:15:53Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Sir ek machine learning ki full playlist provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@ashishahujaindia</td>\n",
       "      <td>2024-03-08T16:06:59Z</td>\n",
       "      <td>2024-03-08T16:07:34Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Thanks CodeWithHarry. Bro, tum samaaj sewi ho!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@srilekhach</td>\n",
       "      <td>2024-03-07T14:46:01Z</td>\n",
       "      <td>2024-03-07T14:46:01Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi Harry,&lt;br&gt;Your video is really impressive a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@chandanmishra8925</td>\n",
       "      <td>2024-02-28T13:51:45Z</td>\n",
       "      <td>2024-02-28T13:51:45Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi bro its such a beautiful vedio...and you ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@durreshahwar9468</td>\n",
       "      <td>2024-02-21T18:08:57Z</td>\n",
       "      <td>2024-02-21T18:08:57Z</td>\n",
       "      <td>0</td>\n",
       "      <td>I need your help Harry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@user-yy4bd6ld4y</td>\n",
       "      <td>2024-02-10T06:56:42Z</td>\n",
       "      <td>2024-02-10T06:56:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>I want to learn scrapping with beautiful soap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@isrargaming2849</td>\n",
       "      <td>2024-01-28T10:57:54Z</td>\n",
       "      <td>2024-01-28T10:57:54Z</td>\n",
       "      <td>0</td>\n",
       "      <td>i am israr, from pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@muhammadajaihashmi1440</td>\n",
       "      <td>2024-01-18T11:47:44Z</td>\n",
       "      <td>2024-01-18T11:47:44Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Muhammad Ajaib Hashmi Azad Kashmir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@KPGAIRCONDSERVICES</td>\n",
       "      <td>2024-01-15T09:33:59Z</td>\n",
       "      <td>2024-01-15T09:33:59Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Pakistan lahore Excellent video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@humayunbaber4495</td>\n",
       "      <td>2024-01-08T17:24:21Z</td>\n",
       "      <td>2024-01-08T17:24:21Z</td>\n",
       "      <td>0</td>\n",
       "      <td>humayun from pakistan , islamabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@savneshprajapati8222</td>\n",
       "      <td>2024-01-07T06:35:22Z</td>\n",
       "      <td>2024-01-07T06:36:29Z</td>\n",
       "      <td>0</td>\n",
       "      <td>I am Savnesh Daksh, a dedicated Data Scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@techieabd5192</td>\n",
       "      <td>2024-01-06T16:29:21Z</td>\n",
       "      <td>2024-01-06T16:29:21Z</td>\n",
       "      <td>0</td>\n",
       "      <td>how many knows that The Real name of Harry Is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@samiahshabbir557</td>\n",
       "      <td>2024-01-04T10:23:25Z</td>\n",
       "      <td>2024-01-04T10:23:25Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi &lt;br&gt;I need help with my project web scrapin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@parvathis7450</td>\n",
       "      <td>2023-12-28T13:17:46Z</td>\n",
       "      <td>2023-12-28T13:17:46Z</td>\n",
       "      <td>0</td>\n",
       "      <td>edu&lt;br&gt;yentha.pottu&lt;br&gt;boly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     author          published_at            updated_at  \\\n",
       "0                @amariqbal  2024-05-05T11:43:18Z  2024-05-05T11:43:18Z   \n",
       "1           @jishankhan6686  2024-04-03T09:33:44Z  2024-04-03T09:33:44Z   \n",
       "2             @Digitcoin150  2024-04-03T09:07:28Z  2024-04-03T09:07:28Z   \n",
       "3           @raingalaxy6594  2024-03-22T17:09:35Z  2024-03-22T17:09:35Z   \n",
       "4          @sulemanrana1480  2024-03-12T08:56:28Z  2024-03-12T08:56:28Z   \n",
       "5          @sulemanrana1480  2024-03-12T08:56:17Z  2024-03-12T08:56:17Z   \n",
       "6          @rahulpathak8415  2024-03-08T18:15:53Z  2024-03-08T18:15:53Z   \n",
       "7         @ashishahujaindia  2024-03-08T16:06:59Z  2024-03-08T16:07:34Z   \n",
       "8               @srilekhach  2024-03-07T14:46:01Z  2024-03-07T14:46:01Z   \n",
       "9        @chandanmishra8925  2024-02-28T13:51:45Z  2024-02-28T13:51:45Z   \n",
       "10        @durreshahwar9468  2024-02-21T18:08:57Z  2024-02-21T18:08:57Z   \n",
       "11         @user-yy4bd6ld4y  2024-02-10T06:56:42Z  2024-02-10T06:56:42Z   \n",
       "12         @isrargaming2849  2024-01-28T10:57:54Z  2024-01-28T10:57:54Z   \n",
       "13  @muhammadajaihashmi1440  2024-01-18T11:47:44Z  2024-01-18T11:47:44Z   \n",
       "14      @KPGAIRCONDSERVICES  2024-01-15T09:33:59Z  2024-01-15T09:33:59Z   \n",
       "15        @humayunbaber4495  2024-01-08T17:24:21Z  2024-01-08T17:24:21Z   \n",
       "16    @savneshprajapati8222  2024-01-07T06:35:22Z  2024-01-07T06:36:29Z   \n",
       "17           @techieabd5192  2024-01-06T16:29:21Z  2024-01-06T16:29:21Z   \n",
       "18        @samiahshabbir557  2024-01-04T10:23:25Z  2024-01-04T10:23:25Z   \n",
       "19           @parvathis7450  2023-12-28T13:17:46Z  2023-12-28T13:17:46Z   \n",
       "\n",
       "    like_count                                               text  \n",
       "0            0                         Amar from Punjab, Pakistan  \n",
       "1            0  Can i scrap data of those website which are no...  \n",
       "2            0                      Kon kon 2024 me dekh raha hai  \n",
       "3            1          My name is Rain and I am from Bangladesh.  \n",
       "4            0                              Suleman Muhammad Khan  \n",
       "5            0                                           Pakistan  \n",
       "6            0  Sir ek machine learning ki full playlist provi...  \n",
       "7            0  Thanks CodeWithHarry. Bro, tum samaaj sewi ho!...  \n",
       "8            0  Hi Harry,<br>Your video is really impressive a...  \n",
       "9            0  Hi bro its such a beautiful vedio...and you ar...  \n",
       "10           0                             I need your help Harry  \n",
       "11           0  I want to learn scrapping with beautiful soap ...  \n",
       "12           0                          i am israr, from pakistan  \n",
       "13           0                 Muhammad Ajaib Hashmi Azad Kashmir  \n",
       "14           0                    Pakistan lahore Excellent video  \n",
       "15           0                  humayun from pakistan , islamabad  \n",
       "16           0  I am Savnesh Daksh, a dedicated Data Scientist...  \n",
       "17           0  how many knows that The Real name of Harry Is ...  \n",
       "18           0  Hi <br>I need help with my project web scrapin...  \n",
       "19           0                        edu<br>yentha.pottu<br>boly  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = DEVELOPER_KEY\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=\"uufDGjTuq34\",\n",
    "    maxResults=100\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "comments = []\n",
    "\n",
    "for item in response['items']:\n",
    "    comment = item['snippet']['topLevelComment']['snippet']\n",
    "    comments.append([\n",
    "        comment['authorDisplayName'],\n",
    "        comment['publishedAt'],\n",
    "        comment['updatedAt'],\n",
    "        comment['likeCount'],\n",
    "        comment['textDisplay']\n",
    "    ])\n",
    "\n",
    "df = pd.DataFrame(comments, columns=['author', 'published_at', 'updated_at', 'like_count', 'text'])\n",
    "\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e18e9-fb1f-4061-b466-13625f329cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
